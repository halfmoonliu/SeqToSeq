{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39484736",
   "metadata": {},
   "source": [
    "## Machine Translation Using Transformer Model\n",
    "The application of sequence-to-sequence model and transformers have greatly enhanced the results of machine translation. In this project, I will walk through the steps needed to train a machine translation model using sequence-to-sequence model and tranformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e5e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprt libraries\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bd0a0",
   "metadata": {},
   "source": [
    "## Dataset: English-French Sentence Pair\n",
    "The data set used in this project comes from Tatoeba (https://tatoeba.org/en), a website where people can upload sentences in any languages and contribute thier own versions of translations in other languages. The English-French dataset used here was downloaded from https://www.manythings.org/anki/, where they preprocessed the Tatoeba dataset so that it became a text file of English Frence sentence pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc645b",
   "metadata": {},
   "source": [
    "## Prepare data: Load file\n",
    "The sentence pair is saved in datapath as a .txt file. The following function ingests the files and make them into pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c60c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(lang1, lang2, dataPath, reverse = False):\n",
    "    print(\"Reading dataset...\")\n",
    "    \n",
    "    #open the file and split by lines (\\n)\n",
    "    lines = open(dataPath, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    #split lines into pairs (separated by tab, or \\t) and normalize\n",
    "    \n",
    "    pairs = [[cleanString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse if spesified\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        source_lang = LangDict(lang2)\n",
    "        target_lang = LangDict(lang1)\n",
    "    else:\n",
    "        source_lang = LangDict(lang1)\n",
    "        target_lang = LangDict(lang2)\n",
    "    \n",
    "    return source_lang, target_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382f147",
   "metadata": {},
   "source": [
    "## Preprocessing: Create Token-Index Dictionary \n",
    "When building neural netword models, it is common practice to map words to a number because models can only \"understand\" numbers\". The following LangDict class is used to walk through the wholde dataset and create word-number mapping dictionary and word count dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f693ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token =1\n",
    "\n",
    "class LangDict:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2 # SOS + COS = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e8864",
   "metadata": {},
   "source": [
    "The following functions are used for preprocessing data (e.g. convert text to lowercase, remove puctuations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb58e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def cleanString(s):\n",
    "    #transform letters to lower case ones and remove non-letter symbols\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7582904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_length = 10\n",
    "\n",
    "exn_prefix = (\"i am\", \"i m\",\n",
    "              \"he is\", \"he s\",\n",
    "              \"she is\", \"she s\",\n",
    "              \"you are\", \"you re\",\n",
    "              \"we are\", \"we re\",\n",
    "              \"they are\", \"they re\"              \n",
    "             )\n",
    "\n",
    "def filterPair(p):\n",
    "    p1_tok = p[0].split(' ')\n",
    "    p2_tok = p[1].split(' ')\n",
    "    \n",
    "    if len(p1_tok) < Max_length and len(p2_tok) < Max_length:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def BuildfilterdPairs(pairs):\n",
    "    pairList = list()\n",
    "    for pair in pairs:\n",
    "        if filterPair(pair)==True:\n",
    "            pairList.append([pair[0], pair[1]])\n",
    "    return pairList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8be260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Read 192341 sentence pairs\n",
      "Trimmed to 136657 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 11820\n",
      "fra 19411\n",
      "Below is an example of sentence pair:\n",
      "['they fight like cat and dog .', 'ils se disputent comme chien et chat .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse = False):\n",
    "    dataPath = r'./data/eng-fra.txt'\n",
    "    input_lang, output_lang, pairs = readData(lang1, lang2, dataPath, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = BuildfilterdPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "\n",
    "print(\"Below is an example of sentence pair:\")\n",
    "print(random.choice(pairs))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4683d1",
   "metadata": {},
   "source": [
    "## Encoder Setup\n",
    "Here we define the structure of the first sequence model, the encoder, which encodes the information of the sentence in the source language. The encoder outputs a fixed length embedding, which will serve as the input of the second sequence model, the decoder, to output prediction of word in target language. The type of layer in the sequence model family used for the encoder is to be choosed. Here, I put the GRU (gated recurrent unit) for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf55badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f643fa",
   "metadata": {},
   "source": [
    "## Decoder with Attention\n",
    "Decoder with attention not only takes the hidden state of the encoder as input, but also calculates the product of the current hidden status and outputs of the encoder at each step to enable the decoder to learn to \"Focus its attention\" on different parts of the encoder output to generate translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a29f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length = Max_length):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        ourput, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim =1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4743380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define helper functions\n",
    "def indexFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word]  for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03522232",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bc8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacherForcing_r = 0.5\n",
    "\n",
    "#one training iteration\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "         decoder_optimizer, criterion, max_length = Max_length):\n",
    "    \n",
    "    #initialize encoder hidden layer weights\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    #encoder\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    teacherForcing = True if random.random() < teacherForcing_r else False\n",
    "    #decoder    \n",
    "    if teacherForcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                                                                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                                                            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "        \n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break;\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44263330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function: timers\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s/ 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m,s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3de189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over training process\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every = 100, learning_rate = 0.01):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                     for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                    decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter/ n_iters), \n",
    "                                         iter, iter/ n_iters  *  100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d01fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot resutls\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    #ticks at regular inter\n",
    "    loc = ticker.MultipleLocator(base= 0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cac0a2",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596c3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=Max_length):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "        for ei in range(input_length):\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            \n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device = device)#Start of sentens(SOS)\n",
    "        \n",
    "        decoder_hidden  = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi, = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break;\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7626a0ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14m 28s (- 159m 10s) (5000 8%) 4.4738\n",
      "29m 20s (- 146m 44s) (10000 16%) 4.0735\n",
      "44m 29s (- 133m 27s) (15000 25%) 3.8906\n",
      "59m 39s (- 119m 18s) (20000 33%) 3.7712\n",
      "74m 52s (- 104m 48s) (25000 41%) 3.7296\n",
      "90m 12s (- 90m 12s) (30000 50%) 3.6366\n",
      "105m 27s (- 75m 19s) (35000 58%) 3.6052\n",
      "120m 39s (- 60m 19s) (40000 66%) 3.5151\n",
      "135m 57s (- 45m 19s) (45000 75%) 3.4660\n",
      "151m 17s (- 30m 15s) (50000 83%) 3.4280\n",
      "166m 35s (- 15m 8s) (55000 91%) 3.4521\n",
      "182m 9s (- 0m 0s) (60000 100%) 3.4214\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "atten_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, atten_decoder1, 60000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f64eb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n = 10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Sentence from source language: ', pair[0])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('Model generated sentence: ', output_sentence)\n",
    "        print('Sentence from target language: ', pair[1])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62397de2",
   "metadata": {},
   "source": [
    "## Examples of tranlation produced by current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "559ecd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence from source language:  you ve betrayed us .\n",
      "Model generated sentence:  vous etes au est . <EOS>\n",
      "Sentence from target language:  tu nous as trahies .\n",
      "\n",
      "Sentence from source language:  you re the best singer i know .\n",
      "Model generated sentence:  vous etes la meilleure . . . <EOS>\n",
      "Sentence from target language:  vous etes le meilleur chanteur que je connaisse .\n",
      "\n",
      "Sentence from source language:  it just came out .\n",
      "Model generated sentence:  c est juste de . <EOS>\n",
      "Sentence from target language:  il vient de s avouer homo .\n",
      "\n",
      "Sentence from source language:  this town has narrow streets .\n",
      "Model generated sentence:  ce a a a de la . <EOS>\n",
      "Sentence from target language:  cette ville a des rues etroites .\n",
      "\n",
      "Sentence from source language:  how s your project coming along ?\n",
      "Model generated sentence:  comment va ta . <EOS>\n",
      "Sentence from target language:  comment progresse ton projet ?\n",
      "\n",
      "Sentence from source language:  you d better go to bed .\n",
      "Model generated sentence:  vous mieux mieux de au lit . <EOS>\n",
      "Sentence from target language:  vous feriez mieux d aller vous coucher .\n",
      "\n",
      "Sentence from source language:  tom was killed by a stray bullet .\n",
      "Model generated sentence:  tom a ete ete arrete . <EOS>\n",
      "Sentence from target language:  tom a ete tue par une balle perdue .\n",
      "\n",
      "Sentence from source language:  the room is too big .\n",
      "Model generated sentence:  le piece est trop grand . <EOS>\n",
      "Sentence from target language:  la chambre est trop grande .\n",
      "\n",
      "Sentence from source language:  you made a wise choice .\n",
      "Model generated sentence:  vous es un . . . . . . .\n",
      "Sentence from target language:  vous avez fait un choix judicieux .\n",
      "\n",
      "Sentence from source language:  she shook her head .\n",
      "Model generated sentence:  elle le bebe de . <EOS>\n",
      "Sentence from target language:  elle a secoue la tete .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, atten_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637d983",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), PyTorch tutorial by Sean  Robertson.\n",
    "2. [Aladdin Persson's Github](https://github.com/aladdinpersson)\n",
    "3. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935a81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
