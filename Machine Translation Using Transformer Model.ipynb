{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39484736",
   "metadata": {},
   "source": [
    "## Machine Translation Using Transformer Model\n",
    "The application of sequence-to-sequence model and transformers have greatly enhanced the results of machine translation. In this project, I will walk through the steps needed to train a machine translation model using sequence-to-sequence model and tranformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e5e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprt libraries\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bd0a0",
   "metadata": {},
   "source": [
    "## Dataset: English-French Sentence Pair\n",
    "The data set used in this project comes from Tatoeba (https://tatoeba.org/en), a website where people can upload sentences in any languages and contribute thier own versions of translations in other languages. The English-French dataset used here was downloaded from https://www.manythings.org/anki/, where they preprocessed the Tatoeba dataset so that it became a text file of English Frence sentence pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc645b",
   "metadata": {},
   "source": [
    "## Prepare data: Load file\n",
    "The sentence pair is saved in datapath as a .txt file. The following function ingests the files and make them into pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c60c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(lang1, lang2, dataPath, reverse = False):\n",
    "    print(\"Reading dataset...\")\n",
    "    \n",
    "    #open the file and split by lines (\\n)\n",
    "    lines = open(dataPath, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    #split lines into pairs (separated by tab, or \\t) and normalize\n",
    "    \n",
    "    pairs = [[cleanString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse if spesified\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        source_lang = LangDict(lang2)\n",
    "        target_lang = LangDict(lang1)\n",
    "    else:\n",
    "        source_lang = LangDict(lang1)\n",
    "        target_lang = LangDict(lang2)\n",
    "    \n",
    "    return source_lang, target_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382f147",
   "metadata": {},
   "source": [
    "## Preprocessing: Create Token-Index Dictionary \n",
    "When building neural netword models, it is common practice to map words to a number because models can only \"understand\" numbers\". The following LangDict class is used to walk through the wholde dataset and create word-number mapping dictionary and word count dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f693ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token =1\n",
    "\n",
    "class LangDict:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2 # SOS + COS = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e8864",
   "metadata": {},
   "source": [
    "The following functions are used for preprocessing data (e.g. convert text to lowercase, remove puctuations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb58e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def cleanString(s):\n",
    "    #transform letters to lower case ones and remove non-letter symbols\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7582904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_length = 10\n",
    "\n",
    "exn_prefix = (\"i am\", \"i m\",\n",
    "              \"he is\", \"he s\",\n",
    "              \"she is\", \"she s\",\n",
    "              \"you are\", \"you re\",\n",
    "              \"we are\", \"we re\",\n",
    "              \"they are\", \"they re\"              \n",
    "             )\n",
    "\n",
    "def filterPair(p):\n",
    "    p1_tok = p[0].split(' ')\n",
    "    p2_tok = p[1].split(' ')\n",
    "    \n",
    "    if len(p1_tok) < Max_length and len(p2_tok) < Max_length:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def BuildfilterdPairs(pairs):\n",
    "    pairList = list()\n",
    "    for pair in pairs:\n",
    "        if filterPair(pair)==True:\n",
    "            pairList.append([pair[0], pair[1]])\n",
    "    return pairList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8be260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Read 192341 sentence pairs\n",
      "Trimmed to 136657 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 11820\n",
      "fra 19411\n",
      "Below is an example of sentence pair:\n",
      "['he s coming closer .', 'il s approche .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, dataPath, reverse = False):\n",
    "    input_lang, output_lang, pairs = readData(lang1, lang2, dataPath, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = BuildfilterdPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', r'./data/eng-fra.txt', False)\n",
    "\n",
    "print(\"Below is an example of sentence pair:\")\n",
    "print(random.choice(pairs))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4683d1",
   "metadata": {},
   "source": [
    "## Encoder Setup\n",
    "Here we define the structure of the first sequence model, the encoder, which encodes the information of the sentence in the source language. The encoder outputs a fixed length embedding, which will serve as the input of the second sequence model, the decoder, to output prediction of word in target language. The type of layer in the sequence model family used for the encoder is to be choosed. Here, I put the GRU (gated recurrent unit) for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf55badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f643fa",
   "metadata": {},
   "source": [
    "## Decoder with Attention\n",
    "Decoder with attention not only takes the hidden state of the encoder as input, but also calculates the product of the current hidden status and outputs of the encoder at each step to enable the decoder to learn to \"Focus its attention\" on different parts of the encoder output to generate translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a29f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p = 0.1, max_length = Max_length):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        ourput, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim =1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4743380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define helper functions\n",
    "def indexFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word]  for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03522232",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9bc8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacherForcing_r = 0.5\n",
    "\n",
    "#one training iteration\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "         decoder_optimizer, criterion, max_length = Max_length):\n",
    "    \n",
    "    #initialize encoder hidden layer weights\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    #encoder\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    teacherForcing = True if random.random() < teacherForcing_r else False\n",
    "    #decoder    \n",
    "    if teacherForcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                                                                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                                                            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "        \n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break;\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44263330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function: timers\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s/ 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m,s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3de189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over training process\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every = 100, learning_rate = 0.01):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                     for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                    decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter/ n_iters), \n",
    "                                         iter, iter/ n_iters  *  100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d01fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot resutls\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    #ticks at regular inter\n",
    "    loc = ticker.MultipleLocator(base= 0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cac0a2",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596c3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=Max_length):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "        for ei in range(input_length):\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            \n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device = device)#Start of sentens(SOS)\n",
    "        \n",
    "        decoder_hidden  = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi, = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break;\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7626a0ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110m 1s (- 1210m 21s) (5000 8%) 4.4568\n",
      "125m 15s (- 626m 16s) (10000 16%) 4.0724\n",
      "140m 37s (- 421m 53s) (15000 25%) 3.8844\n",
      "155m 51s (- 311m 43s) (20000 33%) 3.7266\n",
      "171m 16s (- 239m 46s) (25000 41%) 3.6990\n",
      "186m 53s (- 186m 53s) (30000 50%) 3.6056\n",
      "202m 26s (- 144m 36s) (35000 58%) 3.5730\n",
      "217m 54s (- 108m 57s) (40000 66%) 3.4991\n",
      "233m 25s (- 77m 48s) (45000 75%) 3.4696\n",
      "249m 2s (- 49m 48s) (50000 83%) 3.4135\n",
      "264m 40s (- 24m 3s) (55000 91%) 3.3720\n",
      "280m 23s (- 0m 0s) (60000 100%) 3.3676\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "atten_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, atten_decoder1, 60000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f64eb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n = 10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Sentence from source language: ', pair[0])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('Model generated sentence: ', output_sentence)\n",
    "        print('Sentence from target language: ', pair[1])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62397de2",
   "metadata": {},
   "source": [
    "## Examples of tranlation produced by current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "559ecd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence from source language:  the section chief accepted the proposal .\n",
      "Model generated sentence:  le a ete l le la viande . <EOS>\n",
      "Sentence from target language:  le chef de section a accepte la proposition .\n",
      "\n",
      "Sentence from source language:  i need to ask a few questions .\n",
      "Model generated sentence:  je dois m avoir un . <EOS>\n",
      "Sentence from target language:  je dois poser quelques questions .\n",
      "\n",
      "Sentence from source language:  he can t stop laughing .\n",
      "Model generated sentence:  il peut peut pas y . . . . .\n",
      "Sentence from target language:  il n arrive pas a arreter de rire .\n",
      "\n",
      "Sentence from source language:  tom needed food .\n",
      "Model generated sentence:  tom est de la maison <EOS>\n",
      "Sentence from target language:  tom avait besoin de manger .\n",
      "\n",
      "Sentence from source language:  i ve been asked to sing tonight .\n",
      "Model generated sentence:  je n ai demande de de ce soir . <EOS>\n",
      "Sentence from target language:  on m a demande de chanter ce soir .\n",
      "\n",
      "Sentence from source language:  don t leave them alone .\n",
      "Model generated sentence:  ne te laisse pas seule ! <EOS>\n",
      "Sentence from target language:  ne les laissez pas seuls .\n",
      "\n",
      "Sentence from source language:  you are as white as a sheet .\n",
      "Model generated sentence:  vous etes aussi que l . <EOS>\n",
      "Sentence from target language:  vous etes blanc comme un drap .\n",
      "\n",
      "Sentence from source language:  can i buy you another beer ?\n",
      "Model generated sentence:  peux ce que vous autre une <EOS>\n",
      "Sentence from target language:  puis je vous offrir une autre biere ?\n",
      "\n",
      "Sentence from source language:  the hilton hotel please .\n",
      "Model generated sentence:  le la pluie le . <EOS>\n",
      "Sentence from target language:  l hotel hilton s il vous plait .\n",
      "\n",
      "Sentence from source language:  a hungry man is an angry man .\n",
      "Model generated sentence:  une ! <EOS>\n",
      "Sentence from target language:  vilain affame moitie enrage .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, atten_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15505860",
   "metadata": {},
   "source": [
    "## Evaluate the model using BLEU Score\n",
    "One common measure of machine translation performance is the BLEU (Bilingual Evaluation Understudy) score. The BLEU score measures if all the N-grams in the ground truth translation are covered by the generated translation. Consider the following ground truth sentence pair:\n",
    "\n",
    "(Eng) How are you doing - (French) Comment ca va\n",
    "\n",
    "Based on the English sentence, the model generates the following sentence:\n",
    "\n",
    "Tout va bien\n",
    "\n",
    "If we measure the number 1 gram in the ground truth appears in the generated sentence, there is only va, which leads to the BLEU score of 1 / 3 ~ 0.33.\n",
    "\n",
    "If we measure the number 2-grams in the ground truth that appear in the generated sentence, there is zero (Comment ca & ca va), which leads to the BLEU score of 0 / 2 = 0.\n",
    "\n",
    "From this example, we can see the problem of BLEU score. The number of overlapping n-grams is not a perfect measure for translation because 'comment ca va' and ' and 'tout va bien' can both be correct translations for 'how are you doing'. However, the BLEU score is still a widely used measure for machine learning tasks using large amounts of data.\n",
    "\n",
    "Below is how to use a pre-built bleu score module from NLTK to calculate the model performance using the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7b1591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Read 19236 sentence pairs\n",
      "Trimmed to 13660 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 4970\n",
      "fra 7125\n"
     ]
    }
   ],
   "source": [
    "input_lang_test, output_lang_test, pairs_test = prepareData('eng', 'fra', r'./data/eng-fra_test.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7c721fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halfmoonliu/opt/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/halfmoonliu/opt/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/halfmoonliu/opt/anaconda3/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score for unseen test dataset is:\n",
      "0.4719676940558071\n",
      "Number of pairs with no BLEU Score:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Calculate BLEU score (1-gram)\n",
    "from nltk import translate\n",
    "\n",
    "TotalBleuScore = 0\n",
    "BleuScoreNum = 0\n",
    "NoBleuScoreNum = 0\n",
    "\n",
    "\n",
    "for i in range(1, len(pairs_test)):\n",
    "    \n",
    "    pair = pairs_test[i]\n",
    "    try:\n",
    "        output_words, attentions = evaluate(encoder1, atten_decoder1, pair[0])\n",
    "        predicted_sentence = ' '.join(output_words[:-1])\n",
    "        groundTruth = pair[1]\n",
    "        \n",
    "        TotalBleuScore += translate.bleu_score.sentence_bleu(groundTruth, predicted_sentence, weights =(1, 0, 0, 0))\n",
    "        BleuScoreNum += 1\n",
    "    except:\n",
    "        NoBleuScoreNum += 1\n",
    "        \n",
    "\n",
    "TotalBleuScore /= (len(pairs_test)-1)\n",
    "\n",
    "print(\"Average BLEU score for unseen test dataset is:\")\n",
    "print(TotalBleuScore)\n",
    "\n",
    "print(\"Number of pairs with no BLEU Score:\")\n",
    "print(NoBleuScoreNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637d983",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), PyTorch tutorial by Sean  Robertson.\n",
    "2. [Aladdin Persson's Github](https://github.com/aladdinpersson)\n",
    "3. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n",
    "4. K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. (2002). BLEU: a method for automatic evaluation of machine translation. In ACL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935a81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
