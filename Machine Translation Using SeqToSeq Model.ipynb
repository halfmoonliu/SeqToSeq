{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39484736",
   "metadata": {},
   "source": [
    "## Machine Translation Using SeqToSeq Model\n",
    "The application of sequence-to-sequence model and transformers have greatly enhanced the results of machine translation. In this project, I will walk through the steps needed to train a machine translation model using sequence-to-sequence model and tranformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e5e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprt libraries\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bd0a0",
   "metadata": {},
   "source": [
    "## Dataset: English-French Sentence Pair\n",
    "The data set used in this project comes from Tatoeba (https://tatoeba.org/en), a website where people can upload sentences in any languages and contribute thier own versions of translations in other languages. The English-French dataset used here was downloaded from https://www.manythings.org/anki/, where they preprocessed the Tatoeba dataset so that it became a text file of English Frence sentence pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc645b",
   "metadata": {},
   "source": [
    "## Prepare data: Load file\n",
    "The sentence pair is saved in datapath as a .txt file. The following function ingests the files and make them into pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c60c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(lang1, lang2, dataPath, reverse = False):\n",
    "    print(\"Reading dataset...\")\n",
    "    \n",
    "    #open the file and split by lines (\\n)\n",
    "    lines = open(dataPath, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    #split lines into pairs (separated by tab, or \\t) and normalize\n",
    "    \n",
    "    pairs = [[cleanString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse if spesified\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        source_lang = LangDict(lang2)\n",
    "        target_lang = LangDict(lang1)\n",
    "    else:\n",
    "        source_lang = LangDict(lang1)\n",
    "        target_lang = LangDict(lang2)\n",
    "    \n",
    "    return source_lang, target_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382f147",
   "metadata": {},
   "source": [
    "## Preprocessing: Create Token-Index Dictionary \n",
    "When building neural netword models, it is common practice to map words to a number because models can only \"understand\" numbers\". The following LangDict class is used to walk through the wholde dataset and create word-number mapping dictionary and word count dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f693ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token =1\n",
    "\n",
    "class LangDict:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2 # SOS + COS = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e8864",
   "metadata": {},
   "source": [
    "The following functions are used for preprocessing data (e.g. convert text to lowercase, remove puctuations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb58e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def cleanString(s):\n",
    "    #transform letters to lower case ones and remove non-letter symbols\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7582904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_length = 10\n",
    "\n",
    "exn_prefix = (\"i am\", \"i m\",\n",
    "              \"he is\", \"he s\",\n",
    "              \"she is\", \"she s\",\n",
    "              \"you are\", \"you re\",\n",
    "              \"we are\", \"we re\",\n",
    "              \"they are\", \"they re\"              \n",
    "             )\n",
    "\n",
    "def filterPair(p):\n",
    "    p1_tok = p[0].split(' ')\n",
    "    p2_tok = p[1].split(' ')\n",
    "    \n",
    "    if len(p1_tok) < Max_length and len(p2_tok) < Max_length:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def BuildfilterdPairs(pairs):\n",
    "    pairList = list()\n",
    "    for pair in pairs:\n",
    "        if filterPair(pair)==True:\n",
    "            pairList.append([pair[0], pair[1]])\n",
    "    return pairList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8be260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Read 153873 sentence pairs\n",
      "Trimmed to 109310 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 11027\n",
      "fra 17859\n",
      "Below is an example of sentence pair:\n",
      "['what do you want for breakfast ?', 'que veux tu pour le petit dejeuner ?']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, dataPath, reverse = False):\n",
    "    input_lang, output_lang, pairs = readData(lang1, lang2, dataPath, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = BuildfilterdPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', r'./data/eng-fra_train.txt', False)\n",
    "\n",
    "print(\"Below is an example of sentence pair:\")\n",
    "print(random.choice(pairs))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4683d1",
   "metadata": {},
   "source": [
    "## Encoder Setup\n",
    "Here we define the structure of the first sequence model, the encoder, which encodes the information of the sentence in the source language. The encoder outputs a fixed length embedding, which will serve as the input of the second sequence model, the decoder, to output prediction of word in target language. The type of layer in the sequence model family used for the encoder is to be choosed. Here, I put the GRU (gated recurrent unit) for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf55badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059f3e2",
   "metadata": {},
   "source": [
    "## Decoder Setup\n",
    "The decoder is also a RNN model, which takes the hidden layer of the encoder as input to generate sentence word by work in the target language. The sentence in source language is represented with a set of numbers. This is helpful because often translation cannot be done in word by word fasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0746e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim =1)\n",
    "\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4743380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define helper functions\n",
    "def indexFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word]  for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03522232",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9bc8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacherForcing_r = 0.5\n",
    "\n",
    "#one training iteration\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "         decoder_optimizer, criterion, max_length = Max_length):\n",
    "    \n",
    "    #initialize encoder hidden layer weights\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    #encoder\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    teacherForcing = True if random.random() < teacherForcing_r else False\n",
    "    #decoder    \n",
    "    if teacherForcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "        \n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break;\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44263330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function: timers\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s/ 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m,s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3de189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over training process\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every = 100, learning_rate = 0.01):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                     for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                    decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter/ n_iters), \n",
    "                                         iter, iter/ n_iters  *  100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01d01fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot resutls\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    #ticks at regular inter\n",
    "    loc = ticker.MultipleLocator(base= 0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cac0a2",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596c3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=Max_length):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "        for ei in range(input_length):\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "            \n",
    "            \n",
    "        decoder_input = torch.tensor([[SOS_token]], device = device)#Start of sentens(SOS)\n",
    "        \n",
    "        decoder_hidden  = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi, = decoder_output.data.topk(1)\n",
    "            \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break;\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7626a0ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14m 5s (- 155m 1s) (5000 8%) 4.7450\n",
      "27m 44s (- 138m 43s) (10000 16%) 4.1841\n",
      "41m 21s (- 124m 5s) (15000 25%) 3.8686\n",
      "55m 10s (- 110m 20s) (20000 33%) 3.6714\n",
      "69m 35s (- 97m 26s) (25000 41%) 3.5068\n",
      "83m 41s (- 83m 41s) (30000 50%) 3.3660\n",
      "97m 41s (- 69m 46s) (35000 58%) 3.2737\n",
      "111m 38s (- 55m 49s) (40000 66%) 3.1905\n",
      "125m 41s (- 41m 53s) (45000 75%) 3.0839\n",
      "140m 2s (- 28m 0s) (50000 83%) 2.9935\n",
      "154m 49s (- 14m 4s) (55000 91%) 2.9505\n",
      "169m 53s (- 0m 0s) (60000 100%) 2.8866\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters(encoder1, decoder1, 60000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f64eb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n = 10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('Sentence from source language: ', pair[0])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('Model generated sentence: ', output_sentence)\n",
    "        print('Sentence from target language: ', pair[1])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62397de2",
   "metadata": {},
   "source": [
    "## Examples of tranlation produced by current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "559ecd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence from source language:  we re not family .\n",
      "Model generated sentence:  nous ne sommes pas . <EOS>\n",
      "Sentence from target language:  nous ne sommes pas de la meme famille .\n",
      "\n",
      "Sentence from source language:  tom is heavily armed .\n",
      "Model generated sentence:  tom est a . . <EOS>\n",
      "Sentence from target language:  tom est lourdement arme .\n",
      "\n",
      "Sentence from source language:  thanks for your hard work .\n",
      "Model generated sentence:  merci pour ton travail . <EOS>\n",
      "Sentence from target language:  merci d avoir travaille si durement .\n",
      "\n",
      "Sentence from source language:  i want to be alone for a while .\n",
      "Model generated sentence:  je veux juste un moment pour un moment . <EOS>\n",
      "Sentence from target language:  je veux etre seule un moment .\n",
      "\n",
      "Sentence from source language:  tom has to go even if it rains .\n",
      "Model generated sentence:  tom a a y a y y . <EOS>\n",
      "Sentence from target language:  tom doit y aller meme s il pleut .\n",
      "\n",
      "Sentence from source language:  how s your family ?\n",
      "Model generated sentence:  comment est ta famille ? <EOS>\n",
      "Sentence from target language:  comment va ta famille ?\n",
      "\n",
      "Sentence from source language:  i m blind .\n",
      "Model generated sentence:  je suis suis . <EOS>\n",
      "Sentence from target language:  je suis aveugle .\n",
      "\n",
      "Sentence from source language:  maybe we should talk .\n",
      "Model generated sentence:  nous devrions nous devrions nous devrions . <EOS>\n",
      "Sentence from target language:  peut etre devrions nous parler .\n",
      "\n",
      "Sentence from source language:  the wind calmed down .\n",
      "Model generated sentence:  le le s est . <EOS>\n",
      "Sentence from target language:  le vent s est calme .\n",
      "\n",
      "Sentence from source language:  this is depressing .\n",
      "Model generated sentence:  c est un . . <EOS>\n",
      "Sentence from target language:  c est deprimant .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958252a",
   "metadata": {},
   "source": [
    "## Evaluate the model using BLEU Score\n",
    "One common measure of machine translation performance is the BLEU (Bilingual Evaluation Understudy) score. The BLEU score measures if all the N-grams in the ground truth translation are covered by the generated translation. Consider the following ground truth sentence pair:\n",
    "\n",
    "(Eng) How are you doing - (French) Comment ca va\n",
    "\n",
    "Based on the English sentence, the model generates the following sentence:\n",
    "\n",
    "Tout va bien\n",
    "\n",
    "If we measure the number 1 gram in the ground truth appears in the generated sentence, there is only va, which leads to the BLEU score of 1 / 3 ~ 0.33.\n",
    "\n",
    "If we measure the number 2-grams in the ground truth that appear in the generated sentence, there is zero (Comment ca & ca va), which leads to the BLEU score of 0 / 2 = 0.\n",
    "\n",
    "From this example, we can see the problem of BLEU score. The number of overlapping n-grams is not a perfect measure for translation because 'comment ca va' and ' and 'tout va bien' can both be correct translations for 'how are you doing'. However, the BLEU score is still a widely used measure for machine learning tasks using large amounts of data.\n",
    "\n",
    "Below is how to use a pre-built bleu score module from NLTK to calculate the model performance using the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba919058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Read 19236 sentence pairs\n",
      "Trimmed to 13660 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 4970\n",
      "fra 7125\n"
     ]
    }
   ],
   "source": [
    "input_lang_test, output_lang_test, pairs_test = prepareData('eng', 'fra', r'./data/eng-fra_test.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45f7574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score for unseen test dataset is:\n",
      "0.43858213061744744\n",
      "Number of pairs with no BLEU Score:\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "#Calculate BLEU score (1-gram)\n",
    "from nltk import translate\n",
    "\n",
    "TotalBleuScore = 0\n",
    "BleuScoreNum = 0\n",
    "NoBleuScoreNum = 0\n",
    "\n",
    "\n",
    "for i in range(1, len(pairs_test)):\n",
    "    \n",
    "    pair = pairs_test[i]\n",
    "    try:\n",
    "        output_words, attentions = evaluate(encoder1, decoder1, pair[0])\n",
    "        predicted_sentence = ' '.join(output_words[:-1])\n",
    "        groundTruth = pair[1]\n",
    "        \n",
    "        TotalBleuScore += translate.bleu_score.sentence_bleu(groundTruth, predicted_sentence, weights =(1, 0, 0, 0))\n",
    "        BleuScoreNum += 1\n",
    "    except:\n",
    "        NoBleuScoreNum += 1\n",
    "        \n",
    "\n",
    "TotalBleuScore /= (len(pairs_test)-1)\n",
    "\n",
    "print(\"Average BLEU score for unseen test dataset is:\")\n",
    "print(TotalBleuScore)\n",
    "\n",
    "print(\"Number of pairs with no BLEU Score:\")\n",
    "print(NoBleuScoreNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637d983",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), PyTorch tutorial by Sean  Robertson.\n",
    "2. [Aladdin Persson's Github](https://github.com/aladdinpersson)\n",
    "3. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n",
    "4. K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. (2002). BLEU: a method for automatic evaluation of machine translation. In ACL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935a81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
