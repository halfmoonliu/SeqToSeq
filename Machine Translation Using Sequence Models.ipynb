{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39484736",
   "metadata": {},
   "source": [
    "## Machine Translation\n",
    "The application of sequence-to-sequence model and transformers have greatly enhanced the results of machine translation. In this project, I will walk through the steps needed to train a machine translation model using sequence-to-sequence model and tranformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e5e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprt libraries\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bd0a0",
   "metadata": {},
   "source": [
    "## Dataset: English-French Sentence Pair\n",
    "The data set used in this project comes from Tatoeba (https://tatoeba.org/en), a website where people can upload sentences in any languages and contribute thier own versions of translations in other languages. The English-French dataset used here was downloaded from https://www.manythings.org/anki/, where they preprocessed the Tatoeba dataset so that it became a text file of English Frence sentence pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc645b",
   "metadata": {},
   "source": [
    "## Prepare data: Load file\n",
    "The sentence pair is saved in datapath as a .txt file. The following function ingests the files and make them into pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c60c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(lang1, lang2, dataPath, reverse = False):\n",
    "    print(\"Reading dataset...\")\n",
    "    \n",
    "    #open the file and split by lines (\\n)\n",
    "    lines = open(dataPath, encoding = 'utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    #split lines into pairs (separated by tab, or \\t) and normalize\n",
    "    \n",
    "    pairs = [[cleanString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse if spesified\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        source_lang = LangDict(lang2)\n",
    "        target_lang = LangDict(lang1)\n",
    "    else:\n",
    "        source_lang = LangDict(lang1)\n",
    "        target_lang = LangDict(lang2)\n",
    "    \n",
    "    return source_lang, target_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382f147",
   "metadata": {},
   "source": [
    "## Preprocessing: Create Token-Index Dictionary \n",
    "When building neural netword models, it is common practice to map words to a number because models can only \"understand\" numbers\". The following LangDict class is used to walk through the wholde dataset and create word-number mapping dictionary and word count dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f693ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token =1\n",
    "\n",
    "class LangDict:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2 # SOS + COS = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e8864",
   "metadata": {},
   "source": [
    "The following functions are used for preprocessing data (e.g. convert text to lowercase, remove puctuations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb58e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def cleanString(s):\n",
    "    #transform letters to lower case ones and remove non-letter symbols\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7582904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_length = 10\n",
    "\n",
    "exn_prefix = (\"i am\", \"i m\",\n",
    "              \"he is\", \"he s\",\n",
    "              \"she is\", \"she s\",\n",
    "              \"you are\", \"you re\",\n",
    "              \"we are\", \"we re\",\n",
    "              \"they are\", \"they re\"              \n",
    "             )\n",
    "\n",
    "def filterPair(p):\n",
    "    p1_tok = p[0].split(' ')\n",
    "    p2_tok = p[1].split(' ')\n",
    "    \n",
    "    if len(p1_tok) < Max_length and len(p2_tok) < Max_length:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def BuildfilterdPairs(pairs):\n",
    "    pairList = list()\n",
    "    for pair in pairs:\n",
    "        if filterPair(pair)==True:\n",
    "            pairList.append([pair[0], pair[1]])\n",
    "    return pairList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8be260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Read 46301 sentence pairs\n",
      "Trimmed to 45974 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 5683\n",
      "fra 9741\n",
      "Below is an example of sentence pair:\n",
      "['i m going to leave .', 'je vais partir .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse = False):\n",
    "    dataPath = r'./data/eng-fra_small.txt'\n",
    "    input_lang, output_lang, pairs = readData(lang1, lang2, dataPath, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = BuildfilterdPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "\n",
    "print(\"Below is an example of sentence pair:\")\n",
    "print(random.choice(pairs))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4683d1",
   "metadata": {},
   "source": [
    "## Encoder Setup\n",
    "Here we define the structure of the first sequence model, the encoder, which encodes the information of the sentence in the source language. The encoder outputs a fixed length embedding, which will serve as the input of the second sequence model, the decoder, to output prediction of word in target language. The type of layer in the sequence model family used for the encoder is to be choosed. Here, I put the GRU (gated recurrent unit) for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf55badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, dropout = dropout_rate)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = slef.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059f3e2",
   "metadata": {},
   "source": [
    "## Decoder Setup\n",
    "In the original encoder-decoder setting, the decoder takes the hidden layer of the encoder as input to generate  sentence word by work in the target language. We will define the encoder without attention first and with attention later to compare performances between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0746e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_rate):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size,dropout = dropout_rate)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4743380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define helper functions\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [LangDict.word2index[word]  for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03522232",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bc8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one training iteration\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "         decoder_optimizer, criterion, max_length = Max_length):\n",
    "    \n",
    "    #initialize encoder hidden layer weights\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    #encoder\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    #decoder\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        \n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        \n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break;\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44263330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function: timers\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s/ 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m,s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3de189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over training process\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every = 100, learning_rate = 0.01):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                     for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        \n",
    "        training_pair = traning_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                    decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter/ iters), \n",
    "                                         iter, iter/ n_iters  *  100, print_loss_avg))\n",
    "        \n",
    "        if iter % plot_every ==0:\n",
    "            plot_loss_avg = plot_loss_total/ plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "        showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637d983",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), PyTorch tutorial by Sean  Robertson.\n",
    "2. [Aladdin Persson's Github](https://github.com/aladdinpersson)\n",
    "3. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fe469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
